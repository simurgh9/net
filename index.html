<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2020-07-07 Tue 02:37 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Neural Networks: The Forward Pass</title>
<meta name="generator" content="Org mode">
<meta name="author" content="Tashfeen, Ahmad">
<link rel="stylesheet" type="text/css" href="css/org.css"/>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2020 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<header>
<h1 class="title">Neural Networks: The Forward Pass</h1>
<p class="subtitle">Tashfeen, Ahmad</p>
</header>
<figure id="orgd96fc46">
<img src="./media/russian-dolls.jpg" alt="russian-dolls.jpg" width="85%">

<figcaption><span class="figure-number">Figure 1: </span>Matryoshka dolls</figcaption>
</figure>


<div class="abstract">
<p>
"Believe nothing you hear, and only one-half that you see." [<a href="#poe1850system">4</a>] &#x2013; Edgar Allan Poe
</p>

<p>
Many online resources are found with appendages: "in ten minutes", "made easy" and "from scratch". In hopes of getting through to the reader, most of such resources seem to either oversimplify, hide the fair complexity or only talk about the mathematics/code of the Neural Networks. Or, instead of explaining a Neural Network, they discuss how to use library code to quickly put together one. This alludes the reader into the miss-conception that the mathematics and code of a Neural Network should be of two different interests.
</p>

<p>
In this two-part article<sup><a id="fnr.1" class="footref" href="#fn.1">1</a></sup>, we shall look at the mathematics and the code of a simple Neural Network, a. k. a., a Multi-layer Perceptron. In this first part we start with the math of a Neural Network's input-output dynamics, explain how the equations come to be and then write a Python class which implements them. In the <a href="./two.html">second part</a> we will follow-up with how exactly do we find the correct parameters to use in the input-output dynamics we learned in the first part.
</p>

</div>


<div id="outline-container-org33eadca" class="outline-2">
<h2 id="org33eadca"><span class="section-number-2">1</span> The Function</h2>
<div class="outline-text-2" id="text-1">
<p>
Let's start with a fact that is somewhat calming. A neural network is just a <a href="https://en.wikipedia.org/wiki/Function_(mathematics)">mathematical function</a> which we will denote as \(f\). Even though a function, in our code, we shall implement a Python class<sup><a id="fnr.2" class="footref" href="#fn.2">2</a></sup>,
</p>

<div class="org-src-container">
<label class="org-src-name"><span class="listing-number">Listing 1: </span>Neural network Python class definition</label><pre class="src src-python"><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np

<span style="color: #51afef;">class</span> <span style="color: #ECBE7B;">Network</span>:
  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">__init__</span><span style="color: #51afef;">(</span><span style="color: #51afef;">self</span>, X, y, structure, epochs=20, bt_size=32, eta=0.3<span style="color: #51afef;">)</span>:
    <span style="color: #51afef;">pass</span>
</pre>
</div>

<p>
For now, you can ignore the input variables of the <code>__init__</code> definition. We will talk more about them as they become relevant. Before we start thinking about this function \(f\), we need to think about how to model our problems mathematically.
</p>
</div>
</div>

<div id="outline-container-org35821d3" class="outline-2">
<h2 id="org35821d3"><span class="section-number-2">2</span> A Simple Problem</h2>
<div class="outline-text-2" id="text-2">
<p>
Say the problem is to figure out a way of calculating the area of a square, given its side-length. This problem can be modeled with two numbers: the side-length \(x \in \mathbb{R}\) and an area \(f(x) = y \in \mathbb{R}\). Now we can write the <a href="https://en.wikipedia.org/wiki/Domain_of_a_function">domain</a> and <a href="https://en.wikipedia.org/wiki/Codomain">codomain</a> like this \(f:\mathbb{R}\rightarrow \mathbb{R}\).
</p>

<p>
We have modelled our problem and are aware of what the input and output of \(f\) mean. Say I show you a few example side-lengths and area pairs \((x, y)\), e. g.,
</p>

<p>
\[
   \{(0,0)(1,1)(2,4)(3,9)(4,16)(5,25), ...\}
   \]
</p>

<p>
Can you guess the function? Indeed! it's the polynomial \(f(x) = y = x^2\) and you just did a little bit of <i>machine learning</i> in your head. You looked at correct example inputs and outputs and generalised the idea into a general enough function.
</p>
</div>
</div>

<div id="outline-container-org7a20394" class="outline-2">
<h2 id="org7a20394"><span class="section-number-2">3</span> General Problem Modellings</h2>
<div class="outline-text-2" id="text-3">
<p>
In the above problem, we could model the input and output of our problem with just one number. What if instead of a singular value, our problem is best modelled when the inputs and outputs are lists of ordered values: \((\vec{x}, \vec{y})\) vectors? Then the domain and codomain of \(f\) can be written as \(f:\mathbb{R}^n\rightarrow \mathbb{R}^m\).
</p>

<p>
A subset with \(N\) elements of this domain and codomain or input and output are passed to the <code>Network</code> definition above as the variables <code>X, y</code>. This is our sampled data of known examples. <code>X</code> \(\subset \mathbb{R}^n\) of dimensions/shape \((N, n)\) will be a 2D Numpy array of examples (inputs modelled as row arrays/vectors), where <code>y</code> \(\subset \mathbb{R}^m\) will a Numpy array containing the correct outputs which are also called labels.
</p>
</div>

<div id="outline-container-orgdb517d9" class="outline-3">
<h3 id="orgdb517d9"><span class="section-number-3">3.1</span> Classification with One-hot Encoding</h3>
<div class="outline-text-3" id="text-3-1">
<p>
Sometimes we model problems as <i>classification</i> of examples into discrete <i>classes</i>. This classification is based on certain <i>features</i> of any given example input \(\vec{x} \in\) <code>X</code>. Some \(x_i \in \vec{x}\) is considered a single feature. Therefore, the input \(\vec{x}\) can be thought of as a feature vector. Say our goal is to classify \(\vec{x}\) into three discrete classes. For some \(\vec{x} \in \mathbb{R}^n\), we could have a label \(y \in \{0, 1, 2\}\). But what if we wanted to model the label as the probabilities of \((\vec{x}, y)\) for all \(y \in \{0, 1, 2\}\)? We could rewrite a single label as a vector of probabilities \(\vec{y}\) where the probability that \(\vec{x}\) belongs to class \(i\) is \(y_i \in \vec{y}\). Notice how if we are absolutely certain that for some \(\vec{x}, (\vec{x}, 0)\) then \(\vec{y} = [1, 0, 0]\). Similarly for labels \(1, 2\) the output vectors \(\vec{y}\) will be \([0, 1, 0], [0, 0, 1]\). Such an encoding of labels is also known as <i>one-hot encoding</i>. Let's add this to our Python class<sup><a id="fnr.3" class="footref" href="#fn.3">3</a></sup>.
</p>

<div class="org-src-container">
<label class="org-src-name"><span class="listing-number">Listing 2: </span>Assigning sample example data</label><pre class="src src-python"><span style="color: #51afef;">def</span> <span style="color: #c678dd;">__init__</span><span style="color: #51afef;">(</span><span style="color: #51afef;">self</span>, X, y, structure, epochs=20, bt_size=32, eta=0.3<span style="color: #51afef;">)</span>:
                      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">labels to one-hot arrays for decimal labels</span>
  <span style="color: #51afef;">self</span>.X, <span style="color: #51afef;">self</span>.y = X, np.eye<span style="color: #51afef;">(</span><span style="color: #c678dd;">len</span><span style="color: #c678dd;">(</span><span style="color: #c678dd;">set</span><span style="color: #98be65;">(</span>y<span style="color: #98be65;">)</span><span style="color: #c678dd;">)</span><span style="color: #51afef;">)[</span>y.reshape<span style="color: #c678dd;">(</span>-1<span style="color: #c678dd;">)</span><span style="color: #51afef;">]</span>
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-orgab055ea" class="outline-2">
<h2 id="orgab055ea"><span class="section-number-2">4</span> An Example Network</h2>
<div class="outline-text-2" id="text-4">
<p>
Now that we understand the input and output of \(f\), the question is that for some \(\vec{x}\), how should \(f\) map it to the desired \(\vec{y}\)? This process uses a layered architecture with a set of weights \(\mathcal{W}\) and biases \(\mathbf{b}\) and we call it <i>forward-feeding</i> or the <i>forward-pass</i>. We'll learn about forward feeding, weights and biases with a small running example before we generalise. Don't let figure <a href="#orgf07c4cf">2</a> intimidate you. We'll break it down.
</p>


<figure id="orgf07c4cf">
<object type="image/svg+xml" data="./media/mlp3.svg" class="org-svg" width="60%">
Sorry, your browser does not support SVG.</object>

<figcaption><span class="figure-number">Figure 2: </span>Multi-layer Perceptron with 3 layers</figcaption>
</figure>
</div>
</div>

<div id="outline-container-orgdcd3878" class="outline-2">
<h2 id="orgdcd3878"><span class="section-number-2">5</span> Layered Architecture</h2>
<div class="outline-text-2" id="text-5">
<p>
For now ignore all the edges and labels and just look at the green, blue and red vertices. If without any further explanation I ask you to tell me how many <i>layers</i> are in this network, you might say three. Then, if I ask you to give me the number of <i>neurons</i> in each layer, you might say \([5, 4, 2]\). You'd be right in both cases!
</p>

<p>
The first layer in this three-layered Multi-layer Perceptron is the first green column on the left hand side. This layer corresponds to the length of our example input \(\vec{x} \in \mathbb{R}^5\). After the input layer, we have hidden layers. of which in the figure <a href="#orgf07c4cf">2</a>'s network, there is only one: the blue one with four neurons. Note that even though we have only one hidden layer, it is entirely possible for some other network to have more! After the hidden layers, we'll see the output layer. This layer corresponds to the output \(\vec{y}\). Here we read what the output of our network is after a success forward pass. Thus, the network in figure <a href="#orgf07c4cf">2</a> can be written in the function notation like this, \(f:\mathbb{R}^5\rightarrow \mathbb{R}^2\).
</p>
</div>

<div id="outline-container-org7dbd0fd" class="outline-3">
<h3 id="org7dbd0fd"><span class="section-number-3">5.1</span> Layer Indices Notation</h3>
<div class="outline-text-3" id="text-5-1">
<p>
We use the variable \(l \in \mathbb{N}\) for the index of any particular layer where the \(l\) corresponding to the output layer is capitalised as \(L\). In short, the variable we use for the index of all but last layer is \(l\) and the index of the last layer is \(L\) (e. g., for the network in figure <a href="#orgf07c4cf">2</a>, we know that layer \(l=1\) has five neurons, layer \(l=2\) has four neurons and layer \(l=L=3\) has two neurons). I'll denote the number of neurons in a layer \(l\) as \(|l|\); consequently the number of neurons in the output layer is \(|L|\)<sup><a id="fnr.4" class="footref" href="#fn.4">4</a></sup>.
</p>

<p>
We pass this structure of layers about how many neurons we want per layer as list to the Python class with the variable <code>structure</code>. For the network in figure <a href="#orgf07c4cf">2</a>, <code>structure = [5, 4, 2]</code>.
</p>

<div class="org-src-container">
<label class="org-src-name"><span class="listing-number">Listing 3: </span>Assigning the layer structure</label><pre class="src src-python"><span style="color: #51afef;">def</span> <span style="color: #c678dd;">__init__</span><span style="color: #51afef;">(</span><span style="color: #51afef;">self</span>, X, y, structure, epochs=20, bt_size=32, eta=0.3<span style="color: #51afef;">)</span>:
  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">labels to one-hot array</span>
  <span style="color: #51afef;">self</span>.X, <span style="color: #51afef;">self</span>.y = X, np.eye<span style="color: #51afef;">(</span><span style="color: #c678dd;">len</span><span style="color: #c678dd;">(</span><span style="color: #c678dd;">set</span><span style="color: #98be65;">(</span>y<span style="color: #98be65;">)</span><span style="color: #c678dd;">)</span><span style="color: #51afef;">)[</span>y.reshape<span style="color: #c678dd;">(</span>-1<span style="color: #c678dd;">)</span><span style="color: #51afef;">]</span>
  <span style="color: #51afef;">self</span>.structure = structure
  <span style="color: #51afef;">self</span>.epochs, <span style="color: #51afef;">self</span>.bt_size = epochs, bt_size
  <span style="color: #51afef;">self</span>.eta = 0.3
  <span style="color: #51afef;">self</span>.L = <span style="color: #c678dd;">len</span><span style="color: #51afef;">(</span>structure<span style="color: #51afef;">)</span>
</pre>
</div>

<p>
You can ignore the variables <code>epochs, bt_size, eta</code>. We assigned the structure array and <code>self.L</code>. Remember that due the <i>zero-based-indexing</i> of arrays, the index of the last layer here will be <code>self.L-1</code>.
</p>
</div>
</div>
</div>

<div id="outline-container-orgfe02cf4" class="outline-2">
<h2 id="orgfe02cf4"><span class="section-number-2">6</span> Weights and Biases</h2>
<div class="outline-text-2" id="text-6">
<p>
Now that we understand the vertices/neurons in the layers of a network. We are ready to see how the network \(f\) takes \(\vec{x}\) and feeds it forward through all the layers \(l < L\), arriving at the output layer \(L\). The heart of it all is in matrix multiplication. If you don't recall the basics of it, this is a good time to brush-up. The set \(\mathcal{W}\) is a set of matrices; similarly, the set \(\mathbf{b}\) is a set of vectors. For a network with \(L\) layers, we have \(L-1\) many matrices in \(\mathcal{W}\) and vectors in \(\mathbf{b}\).
</p>
</div>

<div id="outline-container-org5e1ea4d" class="outline-3">
<h3 id="org5e1ea4d"><span class="section-number-3">6.1</span> Activations in Layers</h3>
<div class="outline-text-3" id="text-6-1">
<p>
All layers hold an activation vector \(\vec{a} \in \mathbb{R}^n\). We denote the activation vector of layer \(l\) as \(\vec{a}^{(l)}\). Be cautious. The \((l)\) here is not a power or exponent but the index of the layer whose activation vector is \(\vec{a}^{(l)}\). An activation being held in a certain neuron of a certain layer is then denoted as \(a^{(l)}_i \in \vec{a}^{(l)}\). Notice the edges (arrows) going from layer to layer in the figure <a href="#orgf07c4cf">2</a>? An <i>edge</i> that connects \(i^{th}\) neuron in layer \(l-1\) to \(j^{th}\) neuron in layer \(l\) is representing the element \(w_{ji} \in W^{(l)} \in \mathcal{W}\). By now, you should be feeling more familiar with the anatomy of the network shown in figure <a href="#orgf07c4cf">2</a>.
</p>
</div>
</div>

<div id="outline-container-org463e54c" class="outline-3">
<h3 id="org463e54c"><span class="section-number-3">6.2</span> Propagating Activations Forward</h3>
<div class="outline-text-3" id="text-6-2">
<p>
How do we get the activations in the first (input) layer? We simply let it equal to our input vector,
</p>

<p>
\[
    \vec{a}^{(1)} = \vec{x}
    \]
</p>

<p>
Now that we have \(\vec{a}^{(1)}\), how do we get \(\vec{a}^{(2)}\)? We write \(\vec{a}^{(2)}\) as a function<sup><a id="fnr.5" class="footref" href="#fn.5">5</a></sup> of \(\vec{a}^{(1)}\), the weight matrix \(W^{(2)} \in \mathcal{W}\) and the first bias vector \(\vec{b}^{(2)} \in \mathbf{b}\)<sup><a id="fnr.6" class="footref" href="#fn.6">6</a></sup>.
</p>

<p>
\[
    \vec{a}^{(2)} = \sigma\Big(W^{(2)}\vec{a}^{(1)} + \vec{b}^{(2)}\Big)
    \]
</p>

<p>
There are some subtle observations that must be made here that will help us write the code. We introduced another function \(\sigma\). Since we refer to \(W^{(2)}\vec{a}^{(1)} + \vec{b}^{(2)}\) on its own quite often and there are other notational benefits, let \(\vec{z}^{(2)} = W^{(2)}\vec{a}^{(1)} + \vec{b}^{(2)}\). Pause here and think about what will be the dimensions of \(\vec{z}^{(l)}\) for \(l=2\)? We are going from layer 1 to layer 2 so the dimensions of \(\vec{z}^{(l)}\) must be \((|l|, 1)\). This is just saying that \(\vec{a}^{(2)}\) is a vector with \(|l|\) neurons. This should remind you that for multiplication to be valid between two matrices, the first's number of columns should be equal to the second's number of rows! This means that any \(W^{(l)} \in \mathcal{W}\) that gets you from layer \(l-1\) to \(l\) has dimensions \((|l|, |l-1|)\) and \(\vec{b}^{(l)} \in \mathbf{b}\) has \(|l|\) many elements. Therefore, when we multiply \(W^{(l)}\) with dimensions \((|l|, |l-1|)\) to \(\vec{a}^{(l-1)}\) with dimensions \((|l-1|, 1)\) and add \(\vec{b}^{(l)}\) with dimensions \((|l|, 1)\), we get \(\vec{z}^{(l)}\) with dimensions \((|l|, 1)\).
</p>

<p>
\[
    \overbrace{(|l|, \underbrace{|l-1|) \times (|l-1|}_{\text{Have to be equal.}}, 1)}^\text{Product Dimensions: $(|l|, 1)$}
    \]
</p>

<p>
At this point we let \(\vec{z}^{(l)} = W^{(l)}\vec{a}^{(l-1)} + \vec{b}^{(l)}\) then we have the following equations,
</p>

\begin{align}
  \vec{z}^{(l)} &= W^{(l)}\vec{a}^{(l-1)} + \vec{b}^{(l)}  && \text{Outputs to layer $l$} \\
  \vec{a}^{(l)} & = \sigma(\vec{z}^{(l)})  && \text{Activations of layer $l$} \\
\end{align}
</div>
</div>

<div id="outline-container-orgf906f60" class="outline-3">
<h3 id="orgf906f60"><span class="section-number-3">6.3</span> Random Weights and Biases</h3>
<div class="outline-text-3" id="text-6-3">
<p>
A question that I have sleekly avoided so far is how do we find these so called weights and biases sets \((\mathcal{W}, \mathbf{b})\) that enable the network \(f\) to map \(\vec{x}\) to it's expected \(\vec{y}\). This is where <i>machine learning</i> and sample examples come in, which we passed to our network definition as <code>X, y</code>. For now, we just initialise \((\mathcal{W}, \mathbf{b})\) randomly from a normal distribution. We initialise \((\mathcal{W}, \mathbf{b})\) randomly though with correct dimensions, inferring them from <code>self.structure</code>. Remember how all \(W^{(l)} \in \mathcal{W}\) must have dimensions \((|l|, |l-1|)\) and \(\vec{b}^{(l)} \in \mathbf{b}\) must have \(|l|\) many elements? We just initialise them randomly. Let's finish the definition of <code>__init__</code>.
</p>

<div class="org-src-container">
<label class="org-src-name"><span class="listing-number">Listing 4: </span>Initialising \((\mathcal{W}, \mathbf{b})\) randomly.</label><pre class="src src-python"><span style="color: #51afef;">def</span> <span style="color: #c678dd;">__init__</span><span style="color: #51afef;">(</span><span style="color: #51afef;">self</span>, X, y, structure, epochs=20, bt_size=32, eta=0.3<span style="color: #51afef;">)</span>:
  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">labels to one-hot array</span>
  <span style="color: #51afef;">self</span>.X, <span style="color: #51afef;">self</span>.y = X, np.eye<span style="color: #51afef;">(</span><span style="color: #c678dd;">len</span><span style="color: #c678dd;">(</span><span style="color: #c678dd;">set</span><span style="color: #98be65;">(</span>y<span style="color: #98be65;">)</span><span style="color: #c678dd;">)</span><span style="color: #51afef;">)[</span>y.reshape<span style="color: #c678dd;">(</span>-1<span style="color: #c678dd;">)</span><span style="color: #51afef;">]</span>
  <span style="color: #51afef;">self</span>.structure = structure
  <span style="color: #51afef;">self</span>.epochs, <span style="color: #51afef;">self</span>.bt_size = epochs, bt_size
  <span style="color: #51afef;">self</span>.eta = 0.3
  <span style="color: #51afef;">self</span>.L = <span style="color: #c678dd;">len</span><span style="color: #51afef;">(</span>structure<span style="color: #51afef;">)</span>
  <span style="color: #51afef;">self</span>.Wb = <span style="color: #51afef;">self</span>.random_weights_biases<span style="color: #51afef;">()</span>
  <span style="color: #51afef;">self</span>.W, <span style="color: #51afef;">self</span>.b = <span style="color: #51afef;">self</span>.Wb

<span style="color: #51afef;">def</span> <span style="color: #c678dd;">random_weights_biases</span><span style="color: #51afef;">(</span><span style="color: #51afef;">self</span>, sigma=1, mu=0<span style="color: #51afef;">)</span>:
  <span style="color: #dcaeea;">W</span> = np.empty<span style="color: #51afef;">(</span><span style="color: #51afef;">self</span>.L - 1, dtype=<span style="color: #c678dd;">object</span><span style="color: #51afef;">)</span>
  <span style="color: #dcaeea;">b</span> = np.empty<span style="color: #51afef;">(</span><span style="color: #51afef;">self</span>.L - 1, dtype=<span style="color: #c678dd;">object</span><span style="color: #51afef;">)</span>
  <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span><span style="color: #51afef;">(</span><span style="color: #51afef;">self</span>.L - 1<span style="color: #51afef;">)</span>:
    <span style="color: #dcaeea;">c</span>, <span style="color: #dcaeea;">r</span> = <span style="color: #51afef;">self</span>.structure<span style="color: #51afef;">[</span>i<span style="color: #51afef;">]</span>, <span style="color: #51afef;">self</span>.structure<span style="color: #51afef;">[</span>i + 1<span style="color: #51afef;">]</span>
    <span style="color: #dcaeea;">W</span><span style="color: #51afef;">[</span>i<span style="color: #51afef;">]</span> = sigma * np.random.randn<span style="color: #51afef;">(</span>r, c<span style="color: #51afef;">)</span> + mu
    <span style="color: #dcaeea;">b</span><span style="color: #51afef;">[</span>i<span style="color: #51afef;">]</span> = sigma * np.random.randn<span style="color: #51afef;">(</span>r<span style="color: #51afef;">)</span> + mu

  <span style="color: #dcaeea;">Wb</span> = np.empty<span style="color: #51afef;">(</span>2, dtype=<span style="color: #c678dd;">object</span><span style="color: #51afef;">)</span>
  Wb<span style="color: #51afef;">[</span>0<span style="color: #51afef;">]</span>, <span style="color: #dcaeea;">Wb</span><span style="color: #51afef;">[</span>1<span style="color: #51afef;">]</span> = W, b  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">all weights and biases</span>
  <span style="color: #51afef;">return</span> Wb
</pre>
</div>
</div>
</div>

<div id="outline-container-org8a7aef3" class="outline-3">
<h3 id="org8a7aef3"><span class="section-number-3">6.4</span> Sigmoid Logistic Function</h3>
<div class="outline-text-3" id="text-6-4">
<p>
The new \(\sigma(x) = \frac{1}{1+e^{-x}}\) is the sigmoid function. Its job is to take any real value and map it to \((0,1)\). In other words, \(\sigma\) scales everything to a number between zero and one. This means that we want our activations to be between zero and one. When we pass a vector to \(\sigma\), we mean,
</p>

<p>
\[
    \sigma(\vec{x}) = \big[\sigma(x_1), \sigma(x_2), \sigma(x_3), ... , \sigma(x_n) \big]
    \]
</p>


<figure id="orgc2c64b3">
<object type="image/svg+xml" data="./media/sigmoid-graph.svg" class="org-svg" width="70%">
Sorry, your browser does not support SVG.</object>

<figcaption><span class="figure-number">Figure 3: </span>Sigmoid \(\sigma: \mathbb{R} \rightarrow (0,1)\)</figcaption>
</figure>

<p>
Figure <a href="#orgc2c64b3">3</a> shows a graph [<a href="#thoma2014wikimedia">3</a>] of the sigmoid. Later we'll also be needing the first derivative of the sigmoid function \(\sigma'(x) = \sigma(x)(1-\sigma(x))\), so let's add the sigmoid function with a derivative flag to our code.
</p>

<div class="org-src-container">
<label class="org-src-name"><span class="listing-number">Listing 5: </span>Sigmoid and it's first derivative</label><pre class="src src-python"><span style="color: #51afef;">def</span> <span style="color: #c678dd;">sigmoid</span><span style="color: #51afef;">(</span><span style="color: #51afef;">self</span>, x, derivative=<span style="color: #a9a1e1;">False</span><span style="color: #51afef;">)</span>:
  <span style="color: #dcaeea;">s</span> = <span style="color: #51afef;">lambda</span> x: 1 / <span style="color: #51afef;">(</span>1 + np.exp<span style="color: #c678dd;">(</span>-x<span style="color: #c678dd;">)</span><span style="color: #51afef;">)</span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">noqa: E731</span>
  <span style="color: #51afef;">return</span> s<span style="color: #51afef;">(</span>x<span style="color: #51afef;">)</span> * <span style="color: #51afef;">(</span>1 - s<span style="color: #c678dd;">(</span>x<span style="color: #c678dd;">)</span><span style="color: #51afef;">)</span> <span style="color: #51afef;">if</span> derivative <span style="color: #51afef;">else</span> s<span style="color: #51afef;">(</span>x<span style="color: #51afef;">)</span>
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-orgba39261" class="outline-2">
<h2 id="orgba39261"><span class="section-number-2">7</span> Writing Activations Explicitly</h2>
<div class="outline-text-2" id="text-7">
<p>
  We know that a simple neural network \(f:\mathbb{R}^n \rightarrow \mathbb{R}^m\) starts with \(\vec{a}^{(1)} = \vec{x} \in \mathbb{R}^n\) then performs \(L-1\) matrix multiplications as shown in equation (1) and (2) and arrives at \(\vec{a}^{(L)} = \vec{y} \in \mathbb{R}^m\).
For a network as small as the one shown in figure <a href="#orgf07c4cf">2</a>, we can write out the equations for all of its activations. We will also write out the sets \((\mathcal{W}, \mathbf{b})\) with their correctly shaped elements. Since we have three \(L=3\) layers in the Multi-layer Perceptron of figure <a href="#orgf07c4cf">2</a>, we will have \(L-1 = 3-1 = 2\) weight matrices and bias vectors.
</p>

\begin{align*}
  (\mathcal{W}, \mathbf{b}) & = (\{W^{(2)}_{4,5},W^{(3)}_{2,4}\}, \{\vec{b}^{(2)}, \vec{b}^{(3)}\}) \\
  \vec{a}^{(1)} & = \vec{x} \\
  \vec{z}^{(2)} & = W^{(2)}\vec{a}^{(1)} + \vec{b}^{(2)} \\
               & = W^{(2)}\vec{x} + \vec{b}^{(2)}
                && \text{and} \quad \vec{a}^{(2)} = \sigma(\vec{z}^{(2)}) \\
  \vec{z}^{(3)} & = W^{(3)}\vec{a}^{(2)} + \vec{b}^{(3)} \\
               & = W^{(3)}\sigma(W^{(2)}\vec{x} + \vec{b}^{(2)}) + \vec{b}^{(3)}
                && \text{and} \quad \vec{a}^{(3)} = \sigma(\vec{z}^{(3)}) \\
\end{align*}

<p>
Let's generalise and implement forward feeding of any given \(\vec{x}\). We'll write a subroutine with a flag. When the flag is true, the subroutine will return all the activations \(\vec{a}\) and outputs \(\vec{z}\) caused by forward feeding \(\vec{x}\), when false, it'll just return \(\vec{a}^{(L)}\). 
</p>

<div class="org-src-container">
<label class="org-src-name"><span class="listing-number">Listing 6: </span>Feed Forward, a. k. a., Forward Pass</label><pre class="src src-python"><span style="color: #51afef;">def</span> <span style="color: #c678dd;">forward_pass</span><span style="color: #51afef;">(</span><span style="color: #51afef;">self</span>, example, keep_track=<span style="color: #a9a1e1;">True</span><span style="color: #51afef;">)</span>:
  <span style="color: #dcaeea;">input_layer</span> = example.flatten<span style="color: #51afef;">()</span>
  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">if we only want the output of the network</span>
  <span style="color: #51afef;">if</span> keep_track <span style="color: #51afef;">is</span> <span style="color: #a9a1e1;">False</span>:
    <span style="color: #51afef;">for</span> W, b <span style="color: #51afef;">in</span> <span style="color: #c678dd;">zip</span><span style="color: #51afef;">(</span><span style="color: #51afef;">self</span>.W, <span style="color: #51afef;">self</span>.b<span style="color: #51afef;">)</span>:
      <span style="color: #dcaeea;">input_layer</span> = <span style="color: #51afef;">self</span>.sigmoid<span style="color: #51afef;">(</span>np.dot<span style="color: #c678dd;">(</span>W, input_layer<span style="color: #c678dd;">)</span> + b<span style="color: #51afef;">)</span>
    <span style="color: #51afef;">return</span> input_layer
  <span style="color: #dcaeea;">outputs</span> = np.empty<span style="color: #51afef;">(</span>shape=<span style="color: #51afef;">self</span>.L - 1, dtype=np.<span style="color: #c678dd;">object</span><span style="color: #51afef;">)</span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">z^(l)</span>
  <span style="color: #dcaeea;">activations</span> = np.empty<span style="color: #51afef;">(</span>shape=<span style="color: #51afef;">self</span>.L, dtype=np.<span style="color: #c678dd;">object</span><span style="color: #51afef;">)</span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">a^(l)</span>
  <span style="color: #dcaeea;">activations</span><span style="color: #51afef;">[</span>0<span style="color: #51afef;">]</span> = input_layer
  <span style="color: #51afef;">for</span> W, b, l <span style="color: #51afef;">in</span> <span style="color: #c678dd;">zip</span><span style="color: #51afef;">(</span><span style="color: #51afef;">self</span>.W, <span style="color: #51afef;">self</span>.b, <span style="color: #c678dd;">range</span><span style="color: #c678dd;">(</span><span style="color: #51afef;">self</span>.L - 1<span style="color: #c678dd;">)</span><span style="color: #51afef;">)</span>:
    <span style="color: #dcaeea;">outputs</span><span style="color: #51afef;">[</span>l<span style="color: #51afef;">]</span> = np.dot<span style="color: #51afef;">(</span>W, input_layer<span style="color: #51afef;">)</span> + b
    <span style="color: #dcaeea;">activations</span><span style="color: #51afef;">[</span>l + 1<span style="color: #51afef;">]</span> = <span style="color: #51afef;">self</span>.sigmoid<span style="color: #51afef;">(</span>outputs<span style="color: #c678dd;">[</span>l<span style="color: #c678dd;">]</span><span style="color: #51afef;">)</span>
    <span style="color: #dcaeea;">input_layer</span> = activations<span style="color: #51afef;">[</span>l + 1<span style="color: #51afef;">]</span>
  <span style="color: #51afef;">return</span> outputs, activations
</pre>
</div>
</div>
</div>

<div id="outline-container-org0dac7df" class="outline-2">
<h2 id="org0dac7df"><span class="section-number-2">8</span> Testing Code and Assumptions</h2>
<div class="outline-text-2" id="text-8">
<blockquote>
<p>
Beware of bugs in the above code; I have only proved it correct, not tried it. [<a href="#tichy1995experimental">5</a>] &#x2013; Donald E. Knuth
</p>
</blockquote>

<p>
It's time to test the code we have so far and see if it performs as per our assumptions. Let's give our <code>Network</code> a string representation. We'll print out information about each layer in our network. This means: the number of neurons, shape of the associated weight matrix and number of elements in the associated bias vector.
</p>

<div class="org-src-container">
<label class="org-src-name"><span class="listing-number">Listing 7: </span>String representation of the Network class instance</label><pre class="src src-python"><span style="color: #51afef;">def</span> <span style="color: #c678dd;">__repr__</span><span style="color: #51afef;">(</span><span style="color: #51afef;">self</span><span style="color: #51afef;">)</span>:
  <span style="color: #dcaeea;">ret</span> = <span style="color: #98be65;">''</span>
  <span style="color: #51afef;">for</span> l, W, b <span style="color: #51afef;">in</span> <span style="color: #c678dd;">zip</span><span style="color: #51afef;">(</span><span style="color: #51afef;">self</span>.structure, <span style="color: #51afef;">self</span>.W, <span style="color: #51afef;">self</span>.b<span style="color: #51afef;">)</span>:
    <span style="color: #dcaeea;">ret</span> += <span style="color: #98be65;">'({}: W{} + b{})\n'</span>.<span style="color: #c678dd;">format</span><span style="color: #51afef;">(</span>l, W.shape, b.shape<span style="color: #51afef;">)</span>
  <span style="color: #51afef;">return</span> ret

<span style="color: #51afef;">def</span> <span style="color: #c678dd;">__str__</span><span style="color: #51afef;">(</span><span style="color: #51afef;">self</span><span style="color: #51afef;">)</span>:
  <span style="color: #51afef;">return</span> <span style="color: #51afef;">self</span>.__repr__<span style="color: #51afef;">()</span>
</pre>
</div>

<p>
Let's build the network in figure <a href="#orgf07c4cf">2</a>. We know the number of neurons in each of its layers is 5, 4 and 2. Therefore, we'll let <code>structure = [5, 4, 2]</code>. Even though we won't be training this network just yet, we'll still need to pass some mock examples so we can initialise it. For now we can just put this test code in the same <a href="../src/network.py">file</a> <code>network.py</code> after the <code>Network</code> class definition so we don't have to figure out imports. 
</p>

<div class="org-src-container">
<label class="org-src-name"><span class="listing-number">Listing 8: </span>Testing the network class instance</label><pre class="src src-python"><span style="color: #dcaeea;">X</span>, <span style="color: #dcaeea;">y</span> = np.array<span style="color: #51afef;">(</span><span style="color: #c678dd;">[</span><span style="color: #98be65;">[</span>2, 3, 4, 5, 7<span style="color: #98be65;">]</span><span style="color: #c678dd;">]</span><span style="color: #51afef;">)</span>, np.array<span style="color: #51afef;">(</span><span style="color: #c678dd;">[</span>1, 1, 0, 1, 1<span style="color: #c678dd;">]</span><span style="color: #51afef;">)</span>
<span style="color: #dcaeea;">net</span> = Network<span style="color: #51afef;">(</span>X, y, structure=<span style="color: #c678dd;">[</span>5, 4, 2<span style="color: #c678dd;">]</span><span style="color: #51afef;">)</span>
<span style="color: #51afef;">print</span><span style="color: #51afef;">(</span>net<span style="color: #51afef;">)</span>
</pre>
</div>

<p>
Running <code>python3 path/to/network.py</code> gets us,
</p>

<div class="org-src-container">
<label class="org-src-name"><span class="listing-number">Listing 9: </span>Printing network class instance</label><pre class="src src-shell"><span style="color: #51afef;">(</span>5: W<span style="color: #c678dd;">(</span>4, 5<span style="color: #c678dd;">)</span> + b<span style="color: #c678dd;">(</span>4,<span style="color: #c678dd;">)</span><span style="color: #51afef;">)</span>
<span style="color: #51afef;">(</span>4: W<span style="color: #c678dd;">(</span>2, 4<span style="color: #c678dd;">)</span> + b<span style="color: #c678dd;">(</span>2,<span style="color: #c678dd;">)</span><span style="color: #51afef;">)</span>
</pre>
</div>

<p>
This prints out the correct information about the weights and biases! <code>(5: W(4, 5) + b(4,))</code> says that the first layer with five neurons connects with the second layer with 4 neurons using \(W_{4,5}\) and \(\vec{b}\) with 4 elements. Then, <code>(4: W(2, 4) + b(2,))</code> means that the second layer with four neurons connects with the third layer with 2 neurons using \(W_{2,4}\) and \(\vec{b}\) with 2 elements. So far so good.
</p>

<p>
We can further investigate if the one-hot encoding was done properly and if the shapes of each activation vector \(\vec{a}^{(l)}\) is correct.
</p>


<div class="org-src-container">
<label class="org-src-name"><span class="listing-number">Listing 10: </span>Testing activations \(a^{(l)}\) and one-hot encodings \(\vec{y}\)</label><pre class="src src-python"><span style="color: #51afef;">print</span><span style="color: #51afef;">(</span>net.y<span style="color: #51afef;">)</span>
<span style="color: #dcaeea;">outputs</span>, <span style="color: #dcaeea;">activations</span> = net.forward_pass<span style="color: #51afef;">(</span>np.array<span style="color: #c678dd;">(</span><span style="color: #98be65;">[</span>1, 0, 1, 0, 1<span style="color: #98be65;">]</span><span style="color: #c678dd;">)</span><span style="color: #51afef;">)</span>
<span style="color: #51afef;">for</span> a <span style="color: #51afef;">in</span> activations:
  <span style="color: #51afef;">print</span><span style="color: #51afef;">(</span>a.shape<span style="color: #51afef;">)</span>
</pre>
</div>

<p>
Running <code>python3 path/to/network.py</code> gets us,
</p>

<div class="org-src-container">
<label class="org-src-name"><span class="listing-number">Listing 11: </span>Printing activations \(a^{(l)}\) and one-hot encodings \(\vec{y}\)</label><pre class="src src-shell"><span style="color: #51afef;">[</span><span style="color: #c678dd;">[</span>0. 1.<span style="color: #c678dd;">]</span>
 <span style="color: #c678dd;">[</span>0. 1.<span style="color: #c678dd;">]</span>
 <span style="color: #c678dd;">[</span>1. 0.<span style="color: #c678dd;">]</span>
 <span style="color: #c678dd;">[</span>0. 1.<span style="color: #c678dd;">]</span>
 <span style="color: #c678dd;">[</span>0. 1.<span style="color: #c678dd;">]</span><span style="color: #51afef;">]</span>
<span style="color: #51afef;">(</span>5,<span style="color: #51afef;">)</span>
<span style="color: #51afef;">(</span>4,<span style="color: #51afef;">)</span>
<span style="color: #51afef;">(</span>2,<span style="color: #51afef;">)</span>
</pre>
</div>

<p>
Can you tell why this output is correct?
</p>
</div>
</div>

<div id="outline-container-org6210c57" class="outline-2">
<h2 id="org6210c57"><span class="section-number-2">9</span> Matryoshka Dolls</h2>
<div class="outline-text-2" id="text-9">
<p>
The banner [<a href="#russian2020dolls">1</a>] of this article is of <a href="https://en.wikipedia.org/wiki/Matryoshka_doll">Matryoshka Dolls</a>. Why? They resemble a Multi-layer Perceptron in structure [<a href="#vermont2020dolls">2</a>]. Think of the outermost doll as the input layer. She takes the input vector \(\vec{x}\), <del>performs the forward feeding</del> says her little magic spell to get the next activation and passes it onto the next doll within. When the propagation reaches the doll present at the innermost layer, we uncap all the dolls and ask the innermost doll for the output \(\vec{y}\). What if it's not correct \(\hat{\vec{y}} \neq \vec{y}\)?
</p>


<figure id="org16db05a">
<img src="./media/final-dolls.png" alt="final-dolls.png">

<figcaption><span class="figure-number">Figure 4: </span>Matryoshka Layers</figcaption>
</figure>


<div id="bibliography">
<h2>References</h2>

<table>

<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="russian2020dolls">1</a>]
</td>
<td class="bibtexitem">
Russian cultural experiences.
 [Online; accessed June 28, 2020].
[&nbsp;<a href="citations_bib.html#russian2020dolls">bib</a>&nbsp;| 
<a href="https://www.pinterest.com/pin/203225001910640380/">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="vermont2020dolls">2</a>]
</td>
<td class="bibtexitem">
Russian nesting dolls.
 [Online; accessed June 28, 2020].
[&nbsp;<a href="citations_bib.html#vermont2020dolls">bib</a>&nbsp;| 
<a href="https://www.vermontcountrystore.com/russian-nesting-dolls/product/70072">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="thoma2014wikimedia">3</a>]
</td>
<td class="bibtexitem">
Martin Thoma.
 Wikimedia commons, 2014.
 [Online; accessed June 26, 2020].
[&nbsp;<a href="citations_bib.html#thoma2014wikimedia">bib</a>&nbsp;| 
<a href="https://commons.wikimedia.org/wiki/File:Sigmoid-function-2.svg">http</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="poe1850system">4</a>]
</td>
<td class="bibtexitem">
Edgar&nbsp;Allan Poe.
 <em>The System of Dr. Tarr and Prof. Fether</em>.
 Virginia Tech, 1850.
[&nbsp;<a href="citations_bib.html#poe1850system">bib</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="tichy1995experimental">5</a>]
</td>
<td class="bibtexitem">
Walter&nbsp;F Tichy, Paul Lukowicz, Lutz Prechelt, and Ernst&nbsp;A Heinz.
 Experimental evaluation in computer science: A quantitative study.
 <em>Journal of Systems and Software</em>, 28(1):9--18, 1995.
[&nbsp;<a href="citations_bib.html#tichy1995experimental">bib</a>&nbsp;]

</td>
</tr>
</table>
</div>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1">1</a></sup> <div class="footpara"><p class="footpara">
To report any mistakes or contact me, send an email with the appropriate subject to <i>simurgh9(at)pm.me</i>.
</p></div></div>

<div class="footdef"><sup><a id="fn.2" class="footnum" href="#fnr.2">2</a></sup> <div class="footpara"><p class="footpara">
I will use only vanilla Python 3, with the famous library <a href="https://numpy.org/">Numpy</a> for fast vectorised array operations and linear algebra.
</p></div></div>

<div class="footdef"><sup><a id="fn.3" class="footnum" href="#fnr.3">3</a></sup> <div class="footpara"><p class="footpara">
We're making this <code>Network</code> class keeping classification in mind. Though we can do regression with the final product by changing only a few line.
</p></div></div>

<div class="footdef"><sup><a id="fn.4" class="footnum" href="#fnr.4">4</a></sup> <div class="footpara"><p class="footpara">
I am not sure if denoting the number of neurons in layer \(l\) as \(|l|\) is the standard notation. But, indexing layers with \(l\) is.
</p></div></div>

<div class="footdef"><sup><a id="fn.5" class="footnum" href="#fnr.5">5</a></sup> <div class="footpara"><p class="footpara">
You maybe confused that at the start of we called \(\vec{a}\) a vector and now we are calling it a function? Think of it like this: activations are functions that evaluate to a vector \(\vec{a}^{(l)}\).
</p></div></div>

<div class="footdef"><sup><a id="fn.6" class="footnum" href="#fnr.6">6</a></sup> <div class="footpara"><p class="footpara">
Note that \(W^{l} \in \mathcal{W}\) and \(b^{(l)} \in \mathbf{b}\) are written with the index of a layer in the superscript and not an exponent. Just like the activation \(\vec{a}^{(l)}\).
</p></div></div>


</div>
</div></div>
<div id="postamble" class="status">
<p class="author">Author: Tashfeen, Ahmad</p>
<p class="date">Created: 2020-07-07 Tue 02:37</p>
</div>
</body>
</html>
